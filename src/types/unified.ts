/**
 * Metadata about a question type in a benchmark dataset.
 * Used to display human-readable labels in reports and the web UI.
 */
export interface QuestionTypeInfo {
  /** Raw type identifier from the benchmark (e.g., "1", "single-session-user") */
  id: string
  /** Short human-readable alias (e.g., "single-hop", "temporal") */
  alias: string
  /** Longer description of what this question type tests */
  description: string
}

/** Maps raw question type IDs to their display info. */
export type QuestionTypeRegistry = Record<string, QuestionTypeInfo>

/**
 * A single message in a conversation, normalized across all benchmark formats.
 */
export interface UnifiedMessage {
  /** Speaker role in the conversation */
  role: "user" | "assistant"
  /** Message text content */
  content: string
  /** ISO timestamp of when the message was sent */
  timestamp?: string
  /** Speaker name (if available from the benchmark dataset) */
  speaker?: string
}

/**
 * A conversation session containing a sequence of messages.
 * Sessions are the primary unit of data ingested into memory providers.
 */
export interface UnifiedSession {
  /** Unique session identifier from the benchmark */
  sessionId: string
  /** Ordered list of messages in this session */
  messages: UnifiedMessage[]
  /** Optional benchmark-specific metadata (dates, topics, etc.) */
  metadata?: Record<string, unknown>
}

/**
 * A benchmark question with its ground truth answer.
 * Each question references one or more haystack sessions that contain the answer.
 */
export interface UnifiedQuestion {
  /** Unique question identifier */
  questionId: string
  /** The question text to be answered */
  question: string
  /** Question type ID (maps to QuestionTypeRegistry for display info) */
  questionType: string
  /** Expected correct answer */
  groundTruth: string
  /** IDs of sessions that contain information needed to answer this question */
  haystackSessionIds: string[]
  /** Optional benchmark-specific metadata (e.g., date context for temporal questions) */
  metadata?: Record<string, unknown>
}

/** Provider-agnostic search result type. Each provider returns different shapes. */
export type SearchResult = unknown

/**
 * Per-question retrieval quality metrics computed during evaluation.
 * Measures how well the search results cover the relevant information.
 */
export interface RetrievalMetrics {
  /** Whether any relevant document appeared in top-K results (0 or 1) */
  hitAtK: number
  /** Fraction of top-K results that are relevant */
  precisionAtK: number
  /** Fraction of all relevant documents found in top-K results */
  recallAtK: number
  /** Harmonic mean of precision and recall */
  f1AtK: number
  /** Mean Reciprocal Rank — 1/rank of the first relevant result */
  mrr: number
  /** Normalized Discounted Cumulative Gain — position-aware relevance score */
  ndcg: number
  /** The K value used for these metrics */
  k: number
  /** Count of relevant documents found in the results */
  relevantRetrieved: number
  /** Total number of relevant documents that exist */
  totalRelevant: number
}

/**
 * Aggregated retrieval metrics across multiple questions.
 * Computed by averaging per-question RetrievalMetrics.
 */
export interface RetrievalAggregates {
  /** Average Hit@K across all questions */
  hitAtK: number
  /** Average Precision@K across all questions */
  precisionAtK: number
  /** Average Recall@K across all questions */
  recallAtK: number
  /** Average F1@K across all questions */
  f1AtK: number
  /** Average MRR across all questions */
  mrr: number
  /** Average NDCG across all questions */
  ndcg: number
  /** The K value used for these metrics */
  k: number
}

/**
 * Complete evaluation result for a single question.
 * Contains the question, the provider's answer, the judge's verdict, and timing data.
 */
export interface EvaluationResult {
  questionId: string
  questionType: string
  question: string
  /** Judge score: 1 for correct, 0 for incorrect */
  score: number
  /** Human-readable label derived from score */
  label: "correct" | "incorrect"
  /** Judge's explanation of why the answer was scored this way */
  explanation: string
  /** The answer generated by the answering model based on search results */
  hypothesis: string
  /** The expected correct answer from the benchmark */
  groundTruth: string
  /** Raw search results returned by the provider */
  searchResults: SearchResult[]
  /** Time spent in the search phase (ms) */
  searchDurationMs: number
  /** Time spent generating the answer (ms) */
  answerDurationMs: number
  /** Total time for this question across all phases (ms) */
  totalDurationMs: number
  /** Retrieval quality metrics (if computed) */
  retrievalMetrics?: RetrievalMetrics
}

/**
 * Statistical summary of latency measurements across multiple questions.
 * All values are in milliseconds.
 */
export interface LatencyStats {
  min: number
  max: number
  mean: number
  median: number
  /** 95th percentile latency */
  p95: number
  /** 99th percentile latency */
  p99: number
  /** Standard deviation */
  stdDev: number
  /** Number of measurements */
  count: number
}

/**
 * Accuracy and performance stats for a specific question type.
 * Used in the "BY QUESTION TYPE" section of reports.
 */
export interface QuestionTypeStats {
  /** Total questions of this type */
  total: number
  /** Number answered correctly */
  correct: number
  /** Accuracy ratio (correct / total) */
  accuracy: number
  /** Latency breakdown for this question type */
  latency: {
    search: LatencyStats
    answer: LatencyStats
    total: LatencyStats
  }
  /** Aggregated retrieval metrics for this question type */
  retrieval?: RetrievalAggregates
}

/**
 * Metadata attached to BenchmarkResult when the provider is "ensemble".
 * Captures which providers and strategy were used for the ensemble run.
 */
export interface EnsembleMetadata {
  /** Name of the fusion strategy used (e.g., "rrf", "voting") */
  strategyName: string
  /** Names of the sub-providers in the ensemble */
  subProviders: string[]
  /** Full ensemble configuration as a generic record */
  config: Record<string, unknown>
}

/**
 * Complete benchmark result for a single provider run.
 * Generated by `generateReport()` after the evaluate phase completes.
 * Saved to `data/runs/{runId}/report.json` and displayed in the CLI/web UI.
 */
export interface BenchmarkResult {
  /** Provider that was benchmarked */
  provider: string
  /** Benchmark dataset used (e.g., "locomo", "longmemeval") */
  benchmark: string
  /** Unique run identifier */
  runId: string
  /** Run ID of the data source (for runs that reuse ingested data) */
  dataSourceRunId: string
  /** Judge model used for evaluation (e.g., "gpt-4o") */
  judge: string
  /** Model used for answer generation (e.g., "gpt-4o") */
  answeringModel: string
  /** ISO timestamp of when the report was generated */
  timestamp: string
  /** High-level accuracy summary */
  summary: {
    totalQuestions: number
    correctCount: number
    /** Accuracy ratio (correctCount / totalQuestions) */
    accuracy: number
  }
  /** Latency statistics broken down by pipeline phase */
  latency: {
    ingest: LatencyStats
    indexing: LatencyStats
    search: LatencyStats
    answer: LatencyStats
    evaluate: LatencyStats
    total: LatencyStats
  }
  /** Overall retrieval quality metrics (averaged across all questions) */
  retrieval?: RetrievalAggregates
  /** Per-question-type accuracy and latency breakdown */
  byQuestionType: Record<string, QuestionTypeStats>
  /** Registry mapping question type IDs to display info */
  questionTypeRegistry?: QuestionTypeRegistry
  /** Detailed per-question evaluation results */
  evaluations: EvaluationResult[]
  /** Ensemble-specific metadata (only present for ensemble provider runs) */
  ensembleMetadata?: EnsembleMetadata
}
